{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-24 19:10:40,929 : <ipython-input-3-ed8dff59fd04> : INFO : word2vec start\n",
      "2017-11-24 19:10:40,932 : word2vec : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-11-24 19:10:40,933 : word2vec : INFO : collecting all words and their counts\n",
      "2017-11-24 19:10:40,935 : word2vec : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-24 19:10:40,940 : word2vec : INFO : collected 3149 word types from a corpus of 7658 raw words and 100 sentences\n",
      "2017-11-24 19:10:40,943 : word2vec : INFO : Loading a fresh vocabulary\n",
      "2017-11-24 19:10:40,951 : word2vec : INFO : min_count=1 retains 3149 unique words (100% of original 3149, drops 0)\n",
      "2017-11-24 19:10:40,984 : word2vec : INFO : min_count=1 leaves 7658 word corpus (100% of original 7658, drops 0)\n",
      "2017-11-24 19:10:40,999 : word2vec : INFO : deleting the raw counts dictionary of 3149 items\n",
      "2017-11-24 19:10:41,001 : word2vec : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2017-11-24 19:10:41,004 : word2vec : INFO : downsampling leaves estimated 7414 word corpus (96.8% of prior 7658)\n",
      "2017-11-24 19:10:41,007 : word2vec : INFO : estimated required memory for 3149 words and 100 dimensions: 5983100 bytes\n",
      "2017-11-24 19:10:41,013 : word2vec : INFO : constructing a huffman tree from 3149 words\n",
      "2017-11-24 19:10:41,117 : word2vec : INFO : built huffman tree with maximum node depth 13\n",
      "2017-11-24 19:10:41,124 : word2vec : INFO : resetting layer weights\n",
      "C:\\Users\\1014765\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:906: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-11-24 19:10:41,182 : word2vec : INFO : training model with 3 workers on 3149 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=10\n",
      "2017-11-24 19:12:01,309 : word2vec : INFO : PROGRESS: at 24.60% examples, 117 words/s, in_qsize 3, out_qsize 0\n",
      "2017-11-24 19:12:05,332 : word2vec : INFO : PROGRESS: at 49.20% examples, 226 words/s, in_qsize 2, out_qsize 1\n",
      "2017-11-24 19:12:05,334 : word2vec : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-24 19:12:05,658 : word2vec : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-24 19:12:19,347 : word2vec : INFO : PROGRESS: at 100.00% examples, 378 words/s, in_qsize 0, out_qsize 1\n",
      "2017-11-24 19:12:19,348 : word2vec : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-24 19:12:19,349 : word2vec : INFO : training on 38290 raw words (37063 effective words) took 98.2s, 378 effective words/s\n",
      "2017-11-24 19:12:19,352 : word2vec : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-24 19:12:19,356 : <ipython-input-3-ed8dff59fd04> : INFO : word2vec end\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(module)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import sqlite3\n",
    "from gensim.models import word2vec\n",
    "\n",
    "dbname = \"words.db\"\n",
    "dbcon = sqlite3.connect(dbname)\n",
    "dbcon.row_factory = sqlite3.Row\n",
    "\n",
    "file = open(\"text.txt\", mode=\"w\", encoding=\"utf-8\", newline=\"\\n\")\n",
    "sentences = map(lambda x: x[\"words\"], dbcon.execute(\"select * from wordstbl limit 100\"))\n",
    "for sentence in sentences:\n",
    "    file.write(sentence + \"\\n\")\n",
    "file.close()\n",
    "dbcon.close()\n",
    "\n",
    "logging.info(\"word2vec start\")\n",
    "\n",
    "ls = word2vec.LineSentence('text.txt')\n",
    "model = word2vec.Word2Vec(ls, sg=1, size=100, min_count=1, window=10, hs=1, negative=5)\n",
    "\n",
    "logging.info(\"word2vec end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-24 19:12:48,131 : keyedvectors : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "連動 \t 0.9964715838432312\n",
      "無線通信 \t 0.9960323572158813\n",
      "ビジネス \t 0.9949975609779358\n",
      "音声 \t 0.9948354363441467\n",
      "デュアロ \t 0.9946829676628113\n",
      "真田 \t 0.9945164918899536\n",
      "トレー \t 0.9939086437225342\n",
      "ペッパー \t 0.9936719536781311\n",
      "量 \t 0.9917853474617004\n",
      "セット \t 0.9866690635681152\n"
     ]
    }
   ],
   "source": [
    "results = model.most_similar(positive=\"ソフトバンク\", topn=10)\n",
    "for result in results:\n",
    "    print(result[0], '\\t', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"words.vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

